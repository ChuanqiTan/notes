.. highlight:: cpp

.. role:: raw-html(raw)
   :format: html
.. default-role:: raw-html


################################################
算法导论 读书笔记
################################################


.. sidebar:: 笔记代码开源

    | 项目地址：http://code.google.com/p/introduction-to-algorithms-notes/

.. include:: /head.template


第一部分：基础知识
===============================================


第1章：算法在计算中的作用
-----------------------------------------------

- **算法** 即是一系列的计算步骤，用来将一个有效的输入转换成一个有效的输出。
- 计算机的有限的资源必须被有效的利用，算法就是来解决这些问题的方法。


第2章：算法入门
-----------------------------------------------

- **循环不变式** 的三个性质：（循环不变式通常用来证明递归的正确性）

  - 初始化：它在循环的第一轮迭代开始之前，应该是正确的。
  - 保持：如果在循环的某一次迭代开始之前它是正确的，那么，在下一次迭代开始之前，它也应该保持正确。
  - 终止：当循环结束时，不变式给了我们一个有用的性质，它有助于表明算法是正确的。

- 伪代码中的约定：

  - 书写上的"缩进"表示程序中的分程序（程序块）结构。
  - while,for,repeat等循环结构和if,then,else条件结构与Pascal中相同。
  - 符号 "▷"表示后面部分是个注释。
  - 多重赋值i←j←e是将表达式e的值赋给变量i和j；等价于j←e，再进行赋值i←j。
  - 变量（如i,j和key等）是局部给定过程的。
  - 数组元素是通过"数组名[下标]"这样的形式来访问的。
  - 复合数据一般组织成对象，它们是由属性(attribute)和域(field)所组成的。
  - 参数采用按值传递方式：被调用的过程会收到参数的一份副本。
  - 布尔运算符"and"和"or"都是具有短路能力。

- 算法分析即指对一个算法所需要的资源进行预测。
- 对于一个算法，一般只考察其最坏情况的运行时间，理由有三：

  - 一个算法的最坏情况运行时间是在任何输入下运行时间的一个上界。
  - 对于某些算法来说，最坏情况出现得还是相当频繁的。
  - 大致上看来，"平均情况"通常和最坏情况一样差。
      
- **分治策略** ：将原问题划分成ｎ个规模较小而结构与原问题相似的子问题；递归地解决这些小问题，然后再合并其结果，就得到原问题的解。
- 分治模式在每一层递归上都有三个步骤：

  - 分解(Divde)：将原问题分解成一系列子问题；
  - 解决(Conquer)：递归地解答各子问题。若子问题足够小，则直接求解；
  - 合并(Combine)：将子问题的结果合并成原问题的解。


第3章：函数的增长
-----------------------------------------------
对几个记号的大意：o（非渐近紧确上界） ≈ <； O（渐近上界） ≈ ≤； Θ（渐近紧界）≈ =； Ω（渐近下界）≈ ≥； ω（非渐近紧确下界）≈  >; 这里的<,≤,=,≥,>指的是规模上的比较，即o(g(n))的规模比g(n)小。

  | o(g(n))={ f(n): 对任意正常数c，存在常数n0>0，使对所有的n≧n0，有0≦f(n)<cg(n) }
  | O(g(n))={ f(n): 存在正常数c和n0，使对所有n≧n0，有0≦f(n)≦cg(n) }
  | Θ(g(n))={ f(n):存在正常数c1，c2和n0，使对所有的n≧n0，有0≦c1g(n)≦f(n)≦c2g(n) }
  | Ω(g(n))={ f(n):存在正常数c和n0，使对所有n≧n0，有0≦cg(n)≦f(n) }
  | ω(g(n))={ f(n) 对任意正常数c，存在常数n0>0，使对所有的n≧n0，有0≦cg(n)<f(n) }

    
第4章：递归式
-----------------------------------------------
- **递归式** 是一组等式或不等式，它所描述的函数是用在更小的输入下该函数的值来定义的。
  例如Merge-Sort的最坏情况运行时间T(n)可以用以下递归式来表示：

      T(n) =2T(n/2) + n,   if n > 11,   if n=1

- 解递归式的方法主要有三种：代换法、递归树方法、主方法。
- 代换法(Substitution method)(P38~P40)
    | 定义：先猜测某个界的存在，再用数学归纳法去证明该猜测的正确性。
    | 缺点：只能用于解的形式很容易猜的情形。
    | 总结：这种方法需要经验的积累，可以通过转换为先前见过的类似递归式来求解。
- 递归树方法(Recursion-tree method)
    | 起因：代换法有时很难得到一个正确的好的猜测值。
    | 用途：画出一个递归树是一种得到好猜测的直接方法。
    | 分析(重点)：在递归树中，每一个结点都代表递归函数调用集合中一个子问题的代价。将递归树中每一层内的代价相加得到一个每层代价的集合，再将每层的代价相加得到递归式所有层次的总代价。
    | 总结：递归树最适合用来产生好的猜测，然后用代换法加以验证。

  **递归树的方法非常直观，总的代价就是把所有层次的代价相加起来得到。但是分析这个总代价的规模却不是件很容易的事情，有时需要用到很多数学的知识。**
- 主方法(Master method) 
    **主方法是最好用的Cookbook方法，太神奇了，可以瞬间估计出递归算法的时间复杂度** ，主方法总结了常见的情况并给出了一个公式。
    
  | 优点：针对形如T(n)=aT(n')+f(n)的递归式
  | 缺点：并不能解所有形如上式的递归式的解。因为主方法在第1种情况与第2种情况之间、第2种情况与第3种情况之间都存在着一条沟，所以会存在着不能适用的情况。
  | 直觉上：实际上主方法一直在比较f(n)与 nlogba 的规模，然后选取规模大的作为最后的递归式的规模。

  主方法：设a>=1和b>=1是常数f(n)是定义在非负整数上的一个确定的非负函数。又设T(n)也是定义在非负整数上的一个非负函数，且满足递归方程Tn=aTnb+f(n)
  方程Tn=aTnb+f(n)中的n/b可以是[n/b]，也可以是n/b。那么，在f(n)的三类情况下，我们有T(n)的渐近估计式：

  - 若对于某常数ε>0，有 :math:`f(n)=O(n^{log_b{a-ε}})`  ，则 :math:`T(n)=Θ(n^{log_b{a}})` ；
  - 若 :math:`f(n)=Θ(n^{log_b{a}})`  ，则 :math:`T(n)=Θ(n^{log_b{a}} * logn)` ；
  - 若对其常数ε>0，有 :math:`f(n)=Ω(n^{log_b{a+ε}})`  且对于某常数c>1和所有充分大的正整数n有af(n/b)≤cf(n)，则 :math:`T(n)=θ(f(n))` 。


第5章：概率分析与随机算法
-----------------------------------------------
- 随机算法：如果一个算法的行为不只是由输入决定，同时也由随机数生成器所产生的数值决定，则称这个算法是随机的。
- 指示器随机变量I(A)的定义很简单

  .. math::

    I(A) =
    \begin{cases} 
    0, & 如果A不发生的话 \\
    1, & 如果A发生的话
    \end{cases}

  事件A对应的指示器随机变量的期望期等于事件A发生的概率。

- 介绍了两种 **随机排列数组** 的生成方法：

  - 随机优先级法：为数组的每个元素赋一个随机的优先级，再根据这个优先级对数组中的元素进行排序。可证这样得到的数字满足随机的性质。
  - 原地交换法：依次把A[i]与A[Random(i+1, Length(A))]进行swap，得到的新数组也满足随机性::

      for i ← 1 to n
          do swap A[i] ↔ A[Random(i, Length(A))]

- 在真正的环境中的输入可能并不是随机的，所以我们可以采用先将输入进行随机打乱的方法来保证输入数据的随机性，这点在很多算法中得以体现，比如快排有其随机选取种子数来向输入中加入随机化的成分。
- 生日悖论似乎是很不符合逻辑，但是经过概率分析之后的确如此。
    
  生日悖论非常有意思，理解它的关键在于明白， **在一个23个人的房间内，某2个人生日相同的组合其实有非常多种** 的（23*222＝253种），所以有2人生日相同的概率接近50%；

  如果问题变成：你进入一个22个人的房间，发现里面有人和你生日相同的概率是多少时？，这个概率就非常的低了：
    f=1-(364/365)^22=0.0585713252643433458470990293331

  正确的解法应该是考虑k个人的房间中所有人生日都不相同的概率为：
    f = 1 - (365/365 * 364/365 * ... * (365-n+1)/365) = 0.4756953076625503

  生日悖论也可以这样进行分解：
    1号进入22人房间里有人和他生日相同的概率+2号进入21人房间里有人和他生日相同的概率+3号进入20人房间里有人和他生日相同的概率+…+22号进入1人房间里有人和他生日相同的概率+23号进入0人房间里有人和他生日相同的概率。

  对生日悖论正确理解的关键就是在于明白23个人房间里相同生日的组合非常多，明白了这个组合之后就可以进行正确的分解而不是依赖于直觉进行错误的分解。
- 还值得提一下的是"在线雇佣问题"与"苏格拉底的择偶观"很相似。
    先用三分之一的时间，即分出大、中、小三类，再用三分之一的时间验证自己的观点是否正确，等到最后三分之一时，选择了属于大类中的一支美丽的麦穗。


第二部分：排序和顺序统计学
================================================

- 这一部分将要给出几个解决以下排序问题的算法：

  - 输入：n个数的序列<a1,a2, … an>
  - 输出：输入序列的一个重排<a1’,a2’,...,an’>，使a1’≦a2’≦...≦an’

- 原地排序算法：只有线性个数的元素会被移动到集合之外的排序算法。
- 第6章介绍堆排序
- 第7章介绍快速排序
- 第8章介绍了 **基于"比较"排序的算法的下界为Ω(nlgn)** 。并介绍了几种不基于比较的排序方法，它们能突破Ω(nlgn)的下界。计数排序、基数排序、桶排序。
- 第9章介绍了顺序统计的概念：第i个顺序统计是集合中第i小的数。并介绍了两个算法：

  - 最坏情况为O(n^2)，但平均情况下为线性O(n)的算法
  - 最坏情况下为线性O(n)的算法

.. literalinclude:: source/merge_sort.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/merge_sort.output

.. literalinclude:: source/binary_search.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/binary_search.output



第6章：堆排序
---------------------------------
- 堆排序是一个时间复杂度为O(nlgn)、原地排序算法。
- "堆"数据结构不只在推排序时有用，还可以构成一个有效 **的优先队列** 。
- 堆的定义是这样的：

  - 一个堆是一颗完全二叉树
  - 对于大（小）根堆，每个节点的值都比它的子节点要大（小）

- 虽然堆排的理论效率好，但是往往一个好的快排的实现要优于堆排。
- 所以堆更常见于作为高效的优先级队列（因为它是部分排序的，对于一个优先级队列来说，部分排序已经足够了）：一个堆可以在O(lgn)的时间内，支持大小为n的集合上的任意优先队列的操作。

.. literalinclude:: source/heap_sort.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/heap_sort.output


.. literalinclude:: source/priority_queue.h
    :language: cpp
    :linenos:


第7章：快速排序
---------------------------------
- **快速排序的最坏运行时间为O(n2)，期望运行时间为O(nlgn)** ，且由于O(nlgn)中 **隐含的常数因子很小** ，所以快排通常是用于排序的最佳的实用选择（因为其平均性能非常好）。
- 快排真的太棒了：算法实现简单易懂、平均性能非常好、原地排序不需要额外的空间、算法简单只需要寥寥几行就搞定（比冒泡还少）。
- 对10W个随机数进行排序比较，快排平均在600MS，而堆排平均在900MS，性能差距可见一斑啊。
- 快排的平均情况运行时间与其最佳情况运行时间很接近，而不是非常接近于其最坏情况运行时间，所以一般来说快排效率是最高的，这是快排在现代得以大规模的使用的根本原因。
- **快排很需要随机化技术** ：因为在真正的应用时很容易出现待排序的数组其实已经是有序的情况，而这种已经有序的情况却正好又是快排算法的软肋，它在待排数组有序时的效率是最差的O(n^2)，所以很需要随机化技术！

**快速排序的预随机化** ：正如第5章所说的，由于工程中的输入可能不随机的，所以我们要将其随机化。有两种可选方案

  - 直接对输入数据进行随机化排列
  - 采用随机取样的随机化技术。随机取样的效率更高一些，所以在快速排序的随机化版本中采用随机取样的技术。

  方法很简单，就是在每趟sort之前随机选取一个数与最未尾的元素进行交换操作，这样简单高效的实现了随机化::

      //加入随机取样的随机化技术
      int random_swap = (rand() % (EndIndex - BeginIndex + 1)) + BeginIndex;
      std::swap(ToSort[random_swap], ToSort[EndIndex]);

  这个技术太有用啊，因为快速排序在输入数据已经有序时的性能是最差的，但是 **输入数据已经有序的情况又会经常发生，所以这个随机取样就显得异常的重要** 。如果没有这个随机取样，快排绝得不到这样的应用。
  在我做的实验中，对2000个有序的数据进行排序，在未没采用随机化的情况下，平均耗时860MS，而使用了随机取样之后平均耗时8MS，效率提高了100倍。


.. literalinclude:: source/quick_sort.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/quick_sort.output



第8章：线性时间排序
--------------------------------------------------
- 任何 **比较的排序在最坏的情况下都要用Ω(nlgn)次** 比较来进行排序，所以合并排序和堆排序是渐近最优的。
    注意的是： *快排不是渐近最优的，因为它在最坏的情况下是O(n2)* 。
- 三种以线性时间运行的排序算法：计数排序、基数排序和桶排序。 **它们都是非比较的**
- 计数排序

  - 步骤：统计每一种元素的出现次数，然后再从有序的按出现次数输出，所得的结果就的原数列的有序排列。
  - 计数排序的一个重要性就是它是稳定的排序算法，这个稳定性是基数排序的基石。
  - 计数排序的想法真的很简单、高效、可靠
  - 缺点在于：

    - 需要很多额外的空间（当前类型的值的范围）
    - 只能对离散的类型有效比如int（double就不行了）
    - 基于假设：输入是小范围内的整数构成的。

- 基数排序

  - 步骤：按低纬度到高纬度的把元素放到桶中，再收集起来，完成最高纬度之后，收集的结果就已经是有序的了。
  - 基数排序时对每一维进行调用子排序算法时要求这个子排序算法必须是稳定的。
  - 基数排序与直觉相反：它是按照从底位到高位的顺序排序的。
      我觉得原因在于：高有效位对底有效位有着决定性的作用；后面的排序对前面的排序起决定性的作用。
  - 基于假设：位数有限，并且也是离散的值分布。

- 桶排序

  - 桶排序也 **只是期望运行时间能达到线性** ，对于最坏的情况，它的运行时间取决于它内部使用的子排序算法的运行时间，一般为O(nlgn)。
  - 桶排序基于假设：输入的的元素均匀的分布在区间[0, 1]上。
  - 感觉桶排没有什么大的实现价值，因为它限定了输入的区间，还要求最好是均匀分布，它的最坏情况并不好。

- 所有的 **线性时间内的排序算法，都作出了一定的假设，是建立在一定的假设基础上** 的。

.. literalinclude:: source/linear_sort.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/linear_sort.output



第9章：中位数和顺序统计学
--------------------------------------------------
- 第i个顺序统计量是该集合中第i小的元素。
    最小值是第1个顺序统计量(i=1)最大值是第n个顺序统计量(i=n)
- 中位数是它所在集合的"中点元素"
- **同时找出最大最小值的算法** ，一般人可能以为需要2n次比较， **实际上只需要至多3n/2次比较** ，使用的技巧是：
    将一对元素比较，然后把较大者于max比较，较小者与min比较，这样就只需要3n/2次比较就能得到最后的结果。
- **以期望线性时间选择顺序统计量的方法是以快速排序为模型**

  如同在快速排序中一样，此算法的思想也是对输入数组进行递归划分。 *但和快速排序不同的是，快速排序会递归处理划分的两边，而randomized-select只处理划分的一边* （因为完全没有必要处理另外一边嘛）。
  并由此将期望的运行时间由O(nlgn)下降到了O(n)，但是最坏情况依然是O(n\ :sup:`2` )，虽然这样的最坏情况几乎不会发生，特别是加上随机取样技术之后。 
  
  **这就是顺序统计量算法能够如此高效的核心原因所在！** 我觉得 ``C++ STL中的nth_element`` 用的可能就是这个算法，所以它的效率应该很高。
- 最坏线性时间选择顺序统计量的方法的核心在于：要保证对数组的划分是一个好的划分。
    于是方法使用了一个很奇怪的取主元的方法（而不是直接取最后一个或者随机），虽然看起来很奇怪，但是该方法被这样的提出就肯定有它的理论基础的。
    不过这种取巧的方法不太值得去写一遍，而且明显写出来也很容易的（仅仅相差一个奇怪的取主元方法，这一节的关键内容都在于证明这个方法的最坏线性时间上了），没有什么新技术和新想法。

.. literalinclude:: source/nth_element.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/nth_element.output



第三部分：数据结构
=====================================================


第10章：基本数据结构
--------------------------------------------------
没发现有什么值得看的，貌似就是下面这些基本的知识，这些知识都不知道就没法混啦。

- 栈
- 队列
- 链表
- 树的"左孩子、右兄弟"表示法


第11章：散列表
--------------------------------------------------
- 在散列表中查找一个元素的时间与在链表中查找一个元素的时候相同，在 **最坏情况为O(n)，但期望时间为O(1)**
    在实践中，散列表的效率是很高的，一般可认为是O(1)
- 散列是一种极其有效和实用的技术：基本的字典操作只需要O(1)的平均时间。
  而且当待排序的关键字的集合是静态的（即当关键字集合一旦存入后不需要再改变），"完全散列"能够在O(1)的最坏时间内支持查找操作
- 在众多的简单的解决碰撞的方法中，我觉得比较好的是通过链表法解决碰撞，虽然这个方法的理论最坏效率为O(n)，但是在平均情况下，它的性能也是非常好的，实现简单又高效。
- \ **装载因子** ：给定一个能存放n个元素的、具有m个槽位的散列表T，定义T的装载因子α=n/m，即一个链中平均存储的元素数。
- 多数的散列函数都假定关键字域为自然数集N，如果所给关键字不是自然数，则必须有一种方法来将它们解释为自然数。

  - 除法散列法：h(k)= k mod m
      一般选取m的值为与2的整数幂不大接近的质数
  - 乘法散列法：h(k)= m(kA mod 1)
      构造散列函数的乘法方法包含两个步骤：首先用关键字剩上常数A(0<A<1)，并抽取kA的小数部分；然后用m剩以这个值，再取结果的底。
      Knuth认为A≈5-12是一个比较理想的值。
  - 全域散列：全域散列的基本思想是 **在执行开始时，就从一族仔细设计的函数中，随机地选择一个作为散列函数**

    - 首先：全域散列表是一种使用"键接法"来解决碰撞问题的散列表方法。
    - 随机化保证了对于任何输入，算法都具有较好的平均性能。
    - 全域的散列函数组：设H为一组散列函数，它将给定的关键字域U映射到{0,1,…,m-1}中，这样的一个函数组称为是全域的。如果从H中随机地选择一个散列函数，当关键字K≠J时，两者发生碰撞的概率不大于1/m。
    - 常用的一个全域散列函数类：（ **数论的知识可以证明这个函数类满足全域散列函数的性质，我只要相信这个常用的函数可以被证明就可以了！** ）
        首先选择一个足够大的质数p ，使得每一个可能的关键字k 都落到0 到p-1 的范围内，包括首尾的0 和p-1。
        这里我们假设全域是0 – 15，p 为17。设集合Zp 为{0, 1, 2, …, p-1}，集合Zp* 为{1, 2, 3, …, p}。
        由于p 是质数，我们可以定义散列函数h(a, b, k) = ((a*k + b) mod p) mod m。其中a 属于Zp，b 属于Zp*。
        由所有这样的a 和b 构成的散列函数，组成了函数簇。即全域散列。
    - 明白这个散列函数的选取是在 **执行开始** 随机的选取一个是很重要的，要不然就会不明白到时候怎么进行查找
      这里所谓的随机性应该这样理解：对于某一个散列表来说，它在初始化时已经把a,b固定了，但是对于一个还未初始化的全域散列表来说，a,b是随机选取的。

- 开放寻址法：所有的元素都放在散列表里

    #. 开放寻址法的好处就在于它根本不用指针，而是计算出要存取的各个槽。这样一来，由于不用存储指针就节省了空间，从而可以用同样的空间来提供更多的槽，其潜在的效果就是可以减少碰撞，提高查找速度。
    #. **感觉上开放寻址法很像一个启发式的搜索** ，它的最坏性能也是O(n)，只不过散列函数为它提供了启发信息从而使得一般的平均性能会很好。
    #. 在开放寻址法中，对散列元素的删除操作执行起来比较困难，因为删除操作会影响查找操作。解决办法是在槽里的值被删除后置一个特定的值DELETED，而不是删除后不管，查找的时候处理一下就可以了
    #. 线性探查、二次探查和双重散列都是对最基本的数组法的改进，虽然它们很漂亮，但是思想上并没有太大的革新，看起来很容易懂的。
    #. 双重散钱是用于开放寻址的最好方法之一，因为它所产生的排列具有随机选择的排列的许多特性。
    #. 所有的改进的散列方法目的只有一个： **都是在尽力地增加散列结果的随机性** ！

- **完全散列** ：如果某一种散列技术在进行查找时，其最坏情况内存访问次数为O(1)的话，则称其为完全散列。
    | 书上利用了一种 **两级的散列方案，每一级都采用全域散列** 。
    | 通常利用一种两级的散列方案，每一级上都采用全域散列。为了 **确保在第二级上不出现碰撞** ，需要让第二级散列表Sj的大小mj为散列到槽j中的关键字数nj的平方。
    | 如果利用从某一全域散列函数类中随机选出的散列函数h，来将n个关键字存储到一个大小为m=n的散列表中，并将每个二次散列表的大小置为mj=nj\ :sup:`2` (j=0, 1, …, m-1)，则在一个完全散列方案中，存储所有二次散列表所需的存储总量的期望值小于2n。
    | 完全散列的关键在于：二次散列表中要求没有碰撞！这是通过确保槽的个数是关键字的个数的平方来实现的，第一次散列时要求的槽数为关键字的个数，第二次散列时要求的槽数为当前槽中被分配的关键字的个数的平方。
    | 完全散列的关键在于是对：静态的关键字集合，关键字集合不但不能增加，甚至减少都不行，因为二级散列的槽的个数为散列到该二级散列所在的一级槽中元素的个数的平方。
    | 这种二次散列时要求N2的空间要求似乎感觉会使得完全散列会对空间需求太大，实际上，通过合适的选取第一次的散列函数，总存储空间的预期仍然为O(n)。
- 完全散列的核心在于： **将N个关键值散列到** N\ :sup:`2`\ **个的槽中，就一定可以找到一个无碰撞的散列函数来提供常量级别的查找时间 （空间换时间嘛）** 。

.. literalinclude:: source/hash_table.cpp
   :language: cpp
   :linenos:

.. literalinclude:: output/hash_table.output



第12章：二叉查找树
--------------------------------------------------
- **二叉查找树** 的定义：对任何结点X，其左子树中的关键字最大不超过key[X]；其右子树中的关键字最小不小于key[x]。
- 首先先明显：二叉查找树上的基本操作的时间都与树的高度成正比的，所以高度越小的树性能越高。
- 查询二叉查找树可以考虑使用非递归的版本，它运行要快得多而且也很容易理解::

    SEARCH(x, k):
    while (x != NULL && x.Key != k){
        if (k < x.Key) x = x.Left;
        else x = x.Right;
    }
    return x;

- 前趋和后继：
    | `<S>` 前趋：左一次，然后右到头； `</S>`
    | `<S>` 后继：右一次，然后左到头。 `</S>`
    | **错！不止这么简单，以后继为例：当结点的右子树不存在时，应该一路向上传递，直到找到根结点（没有后继）或者是找到一次非右子树传递（后继找到）为止。我的代码就在这里犯一次错误了，本以为很简单的！**
    | **以上的这2条只对存在左结点（前趋）或右结点（后继）时才有效。**
- 对二叉查找树的插入和删除操作也不复杂，唯一有点难度的地方就是在删除同时存在左右子树的结点时需要进行一些处理。
    书上叙述的有点过度的复杂，其实可以很简单地说明白：对于这样的结点x，找到x结点的前趋（或后继）y，将x的值替换为y的值，然后递归删除y结点就可以了。因为y一定没有右子树（后继对应没有左子树），所以递归删除的时候就是很简单的情况了。
- 可以证明：随机构造的二叉村在平均情况下的行为更接近于最佳情况下的行为，而不是接近最坏情况下的行为。所以一棵在n个关键字上随机构造的二叉查找树的期望高度为O(lgn)。

.. literalinclude:: source/binary_search_tree.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/binary_search_tree.output

.. image:: images/binary_search_tree_output1.png

|

.. image:: images/binary_search_tree_output2.jpg



第13章：红黑树
--------------------------------------------------
满足下面5个条件(红黑性质)的二叉搜索树，称为红黑树：

  #. 每个结点或是红色，或是是黑色。
  #. 根结点是黑的。
  #. 所有的叶结点(NULL)是黑色的。（NULL被视为一个哨兵结点，所有应该指向NULL的指针，都看成指向了NULL结点。）
  #. 如果一个结点是红色的，则它的两个儿子节点都是黑色的。
  #. 对每个结点，从该结点到其子孙结点的所有路径上包含相同数目的黑结点（黑高度相同）。

- 黑高度的定义：从某个结点出发(不包括该结点)到达一个叶结点的任意一条路径上，黑色结点的个数成为该结点x的黑高度。红黑树的黑高度定义为其根结点的黑高度。
- 红黑树是真正的在实际中得到大量应用的复杂数据结构： **C++STL中的关联容器map,set都是红黑树的应用** （所以标准库容器的效率太好了，能用标准库容器尽量使用标准库容器）；Linux内核中的用户态地址空间管理也使用了红黑树。
- 红黑树是许多"平衡的"查找树中的一种（首先：红黑树是一种近似平衡的二叉树），它能保证在最坏的情况下，基本的动态集合操作的时间为O(lgn)。
    红黑树NB的地方就在于它是 **近似平衡，这种近似平衡提高了操作的效率，又不是绝对的平衡，没有带来太多的负作用** 。
- 通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其它路径长出两倍，因而是接近平衡的。
- 全是黑结点的满二叉树也满足红黑树的定义。满二叉树的效率本身就非常高啊，它是效率最好的二叉树了，所以说它是红黑树的一个特例；普通的红黑树要求并没有满二叉树这么严格。
- 旋转操作（左旋和右旋）： **旋转操作是一种能保持二叉查找树性质的查找树局部操作** 。
    所有对红黑树结构的修改都只能通过左右旋来完成，这样才能保证修改后的红黑树首先是一棵二叉查找树。
- **红黑树的插入操作** ：将结点Z插入树T中，就好像T是一棵普通的二叉查找树一样，然后将Z着为红色。为保证红黑性质能继续保持，我们调用一个辅助程序来对结点重新着色并旋转。

  这么做是有它的智慧的：首先，插入结点Z的位置的确应该和普通二叉查找树一样，因为红黑树本身就首先是一棵二叉查找树；
  然后将Z着为红色，是为了保证性质5的正确性，因为性质5如果被破坏了是最难以恢复的；
  到这里，有可能被破坏的性质就只剩下性质2和性质4了，这都可以通过后来的辅助程序进行修复的。

  | 插入操作可能破坏的性质：
  | 性质2：当被一棵空树进行插入操作时发生；
  | 性质4：当新结点被插入到红色结点之后时发生；
- **红黑树的删除操作** ：和插入操作一样，先用BST的删除结点操作，然后调用相应的辅助函数做相应的调整。首先只有被删除的结点为黑结点时才需要进行修补，理由如下：

  - 树中各结点的黑高度都没有变化
  - 不存在两个相邻的红色结点
  - 因为如果被删除的点是红色，就不可能是根，所以根仍然是黑色的

- 当被删除了黑结点之后，红黑树的性质5被破坏，上面说过了性质5被破坏后的修复难度是最大的。
  **这里的修复过程使用了一个很新的思想** ，即视为被删除的结点的子结点有额外的一种黑色，当这一重额外的黑色存在之后，性质5就得到了继续。
  然后再通过转移的方法逐步把这一重额外的黑色逐渐向上转移直到根或者红色的结点，最后消除这一重额外的黑色。

  | 删除操作中可能被破坏的性质：
  | 性质2：当y是根时，且y的一个孩子是红色，若此时这个孩子成为根结点；
  | 性质4：当x和p[y]都是红色时；
  | 性质5：包含y的路径中，黑高度都减少了；
- 红黑树是平衡查找树，还有B树也是另一类平衡查找树。


.. literalinclude:: source/red_black_tree.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/red_black_tree.output

.. image:: images/red_black_tree_output1.png
    :scale: 70%

.. image:: images/red_black_tree_output2.png
    :scale: 80%

.. image:: images/red_black_tree_output3.png
    :scale: 80%

.. image:: images/red_black_tree_output4.png
    :scale: 60%

.. image:: images/red_black_tree_output5.png
    :scale: 80%



第14章：数据结构的扩张
--------------------------------------------------
- 实际的工程中，极少会去创造新的数据结构，通常是对标准的数据结构附加一些信息，并添加一些新的操作以支持应用的要求。
- 数据结构的扩张：指在实际应用数据结构时对标准的数据结构中增加一些信息、编入一些新的操作等等。附加的信息必须能够为该数据结构上的常规操作所更新和维护。
- 对一种数据结构的扩张过程可以分为四个步骤：

  #. 选择基础的数据结构
  #. 确定要在基础数据结构中添加哪些信息
  #. 验证可用基础数据结构上的基本修改操作来维护这些新添加的信息
  #. 设计新的操作

- 红黑树的扩张定理：当结点中新添加的信息可以由该结点和它的左右子树来决定，那么就可以在不影响时间复杂度的前提下在插入和删除等操作中对红黑树的这些附加信息进行维护。



第四部分：高级设计和分析技术
==================================================

- 设计和分析高效算法的 **三种重要技术：动态规划、贪心算法和平摊分析**
- 动态规划通常应用于最优化问题，即要做出一组选择以达到一个最优解时。在做选择的同时，经常出现同样形式的子问题。 **关键技术是存储这些子问题每一个的解，以备它重复出现。**
- 贪心算法通常也是应用于最优化问题，该算法的思想是以局部最优的方式来做每一个选择。采用贪心算法可以比动态规划更快的得出一个最优解，但是关键是不容易判断贪心算法所得到的是否真的是最优解，这是需要证明的，所以说每一个贪心算法后面都有一个漂亮的动态规划算法作为理论支撑。
    贪心算法可以在一定的理论之下，通过只考虑局部最优解就可以保证得到的一定是全局最优解。如赫夫曼编码。
- 平摊分析是一种用来分析执行一系列类似操作的算法的工具。在一个操作序列中，不可能每一个都以其已知的最坏情况运行，某些操作的代价高些，而其它的低一些。

所有的最优化问题都可以通过穷举法来解决，但是这在时间上是不可接受的。所有的高效算法都是为了加快速度：

    - 动态规划：保存子问题的解，以备重复使用
    - 贪心算法：用局部最优解来求全局最优解

平摊分析是用来分析算法的工具，它本身并不是一种算法。


第15章：动态规划
--------------------------------------------------
- 动态规划与分治法之间的区别：

  - 分治法是指将问题分成一些 **独立** 的子问题，递归的求解各子问题
  - 动态规划适用于这些子问题 **独立且重叠** 的情况，也就是各子问题包含公共子问题

- 动态规划算法的设计可以分为4个步骤：

  #. 描述最优解的子结构
  #. 递归定义最优解的值
  #. 按自底向上的方法计算最优解的值
  #. 由计算出的结果反向构造出一个最优解

- \ **动态规划最最最重要的就是要找出最优解的子结构** ！
- 最优子结构在问题域中以两种方式变化（在找出这两个问题的解之后，构造出原问题的最优子结构往往就不是难事了）：

  - 有多少个子问题被用在原问题的一个最优解中
  - 在决定一个最优解中使用哪些子问题有多少个选择

- 动态规划说白了就是一个递归的反向展开的过程：在满足①最优子结构②重叠子问题这2个条件下，通过把递归从下至上的进行展开以避免重复计算子问题从而加速了最终问题的求解的过程。
- \ **再次强调"动态规划最关键的一步就是：寻找最优子结构"**
- 动态规划能够消除重复计算子问题是因为它与普通递归相反，它是通过自下而上的方式来进行求解的。
- 正确使用 **动态规划方法的2个关键要素：最优子结构 和 重叠子问题** 。

  - 如果问题的一个最优解中包含了子问题的最优解，则该问题具有最优子结构；而当一个问题具有最优子结构时，提示我们动态规划可能会适用。
  - 如果问题可以由递归来解决，并且在递归的过程中会不断的出现重复的子问题需要解决，那毫不犹豫的采用动态规划吧！

- 剪贴技术：用来证明在问题的一个最优解中，使用的子问题的解本身也必须是最优的
- 为了描述子问题空间，可以遵循这样一条有效的经验规则，就是尽量保持这个空间简单，然后在需要时再扩充它
- 非正式地：一个动态规划算法的运行时间依赖于两个因素的乘积：子问题的总个数和每个子问题中有多少种选择。
    问题解的代价通常是子问题的代价加上选择本身带来的开销。
- 贪心算法与动态规划有一个显著的区别：就是在贪心算法中，是以自顶向下的方式使用最优子结构的。贪心算法会先做选择，在当时看起来是最优的选择，然后再求解一个结果子问题，而不是先寻找子问题的最优解，然后再选择。
- 要注意：在不能应用最优子结构的时间，就一定不能假设它能够应用。
    **坚决警惕使用动态规划去解决缺乏最优子结构的问题！**
- 使用动态规划时： **子问题必须是相互独立的！** 可以这样理解，N个子问题域互不相干，属于完全不同的空间。
- 重叠子问题：不同的子问题的数目是输入规模的一个多项式。这样，动态规划算法才能充分利用重叠的子问题，减少计算量。即通过每个子问题只解一次，把解保存在一个需要时就可以查看的表中，而每次查表只需要常数时间。

  从这段描述可以看出： **动态规划与递归时做备忘录的本质是完全相同的，所以说备忘录方法与普通的动态递归本质完全相同，没有孰优孰劣之分，哪个方便用哪个。**

- 由计算出的结果反向构造一个最优解：把动态规划或者是递归过程中作出的每一次选择（记住：保存的是每次作出的选择）都保存下来，在最后就一定可以通过这些保存的选择来反向构造出最优解。
- **做备忘录的递归方法** ：这种方法是动态规划的一个变形，它本质上与动态规划是一样的，但是比动态规划更好理解！
  
  - 使用普通的递归结构，自上而下的解决问题。
  - 当在递归算法的执行中每一次遇到一个子问题时，就计算它的解并填入一个表中。以后每次遇到该子问题时，只要查看并返回表中先前填入的值即可。
  - 备忘录方法有一个好处就是：它不会去计算那些对最优解无用的子问题，即只计算必须计算的子问题；但是普通的动态规划会计算所有的子问题，不管它是不是必须的。

- 备忘录方法与动态递归方法的比较：
  
  - 如果所有的子问题都至少要被计算一次，则一个自底向上的动态规划算法通常比一个自顶向下的做备忘录算法好出一个常数因子。因为动态规划没有使用递归的代价，只用到了循环，所以常数因子肯定比递归要好一些。
  - 此外，在有些问题中，还可以用动态规划算法中的表存取模式来进一步的减少时间和空间上的需求；
  - 或者，如果子问题中的某些子问题根本没有必须求解，做备忘录的方法有着只解那些肯定要求解的子问题的优点。（而且这点是自动获得的，那些不必要计算的子问题在备忘录方法中会被自动的抛弃）

- 备忘录方法总结： **由"是否所有的子问题都至少需要被计算一次"来决定使用动态规划还是备忘录**
    再次下定义：这两种方法没孰优孰劣之分，因为它们的本质思想是完全一样的；消除重复子问题。
- 动态规划： **最重要最重要的就是找到最优子结构。在找到最优子结构之后的消除重复子问题，这点我太容易处理了，无论是动态规划的自底向上的递推，还是备忘录，或者是备忘录的变型，都可以轻松的应付。关键就是最优子结构。**


.. literalinclude:: source/longest_common_subsequence.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/longest_common_subsequence.output

.. literalinclude:: source/assemble_dispatch.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/assemble_dispatch.output

.. literalinclude:: source/bitonic_tour.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/bitonic_tour.output

.. literalinclude:: source/plan_party.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/plan_party.output

.. literalinclude:: source/matrix_list_multiply.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/matrix_list_multiply.output

.. literalinclude:: source/best_binary_search_tree.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/best_binary_search_tree.output



第16章：贪心算法
--------------------------------------------------
- 贪心算法使所作的选择看起来都是当前最佳的，期望通所做的局部最优选择来产生一个全局最优解。
    对算法中的每一个决策点，做一个当时看起来是最佳的选择，这种启发式策略并不是总能产生最优解的。
- 贪心算法对大多数优化问题能产生较优解，但很多问题也不能通过这样的局部最优解得到全局最优解（0-1背包）。实际上， **使用贪心算法最关键的就是在于如何证明贪心选择性质以证明当前的问题可以由贪心算法得到最优解**
- 在活动选择问题中，非常关键的一点先决条件就是：活动要先按照结束时间的单调递增顺序排序，这样才能贪心的选择第一个。
    从直觉上来看，这种活动选择方法是一种"贪婪的"选择方法，它给后面剩下的待调度任务留下了尽可能多的机会。也就是说，此处的贪心选择使得剩下的、未调度的时间最大化
- 可以说：动态规划其实是"安全的贪心算法"的基础。无论如何，在 **每一个贪心算法的下面，几乎总是会有一个更加复杂的动态规划解**

  贪心算法实现简单速度快，但是 **证明贪心的正确性往往是很困难的，所以说安全的（能够取得最优解的）贪心算法下面总有一个动态规划算法来证明其正确性**
    所谓安全的贪心算法就是指一定能产生出全局最优解的贪心算法
- 贪心算法的一般步骤（定义）

  - 将优化问题转化成这样的一个问题，即 **先做出选择（对应于动态规划的先解决子问题再选择）** ，再解决剩下的一个子问题。
  - 证明原问题总是有一个最优解是做贪心选择得到的，从而说明贪心选择的安全。
  - 说明在做出贪心选择后，剩余的子问题具有这样的一个性质。即如果将子问题的最优解和我们所做的贪心选择联合起来，就可以得出原问题的一个最优解。

- 正确使用贪心算法的2个关键要素： **贪心选择性质和最优子结构:**

  - 贪心选择性质：一个全局最优解可以通过局部最优（贪心）选择来达到；这也是使用贪心算法中最难的一点，即证明贪心策略得能出最优解。
  - 最优子结构：一个问题的最优解包含了其子问题的最优解。

- 要使用贪心算法就必须先证明以下两个性质：

  - 每一步所做的贪心选择最终能产生一个全局最优解。
      在证明中先考察一个全局最优解，然后证明对该解加以修改，使其采用贪心选择，这个选择将原问题变为一个相似的、但更小的问题。
  - 子问题的最优解与所做的贪心选择合并后，的确可以得到原问题的一个最优解。

- 贪心算法所做的当前选择可能要依赖于已经做出的所有选择，但不依赖于有待于做出的选择或子问题的解。
- 前缀编码：在所有的编码方案中，没有一个编码是另一个编码的前缀。
- **一般地，就算证明不出来贪心算法能给出最优解，但是它一般都至少能给出次优解。所以贪心算法在实际的应用中是非常的普及的。**


.. literalinclude:: source/huffman_code.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/huffman_code.output

.. literalinclude:: source/neatly_print.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/neatly_print.output



第17章：平摊分析
--------------------------------------------------
- 平摊代价： **N个操作的总平摊代价即为这N个操作的总实际代价的上界**
- 在平摊分析中，执行一系列数据结构操作所需要的时间是通过对执行所有操作所花费的时间求平均而得到的。
    平摊分析与平均情况分析的不同之处在于它不牵涉到概率；平摊分析保证在最坏情况下，每个操作具有平均性能。
- 平摊分析表明： **并不能总以最坏情况来衡量算法，因为最坏的情况并不总会发生，而且在绝大多数应用中最坏情况出现的次数一定是很少很少的。**
    最明显的例子就是 ``C++ STL中vector.push_back`` 操作。
- 通过平摊分析，可以获得对某种特定数据结构的认识，这种认识有助于优化设计。
- 三种方法进行平摊分析：

  - 聚集分析：就是指分析一系列操作的总时间
  - 记账法：对每次操作的对象进行预先记账
  - 势能方法：与记账法类似，但是将每次预留的势能视为是整个数据结构共享的

- 聚集分析：由N个操作所构成的序列的总时间在最坏的情况下为T(n)，则每个操作的平均代价（平摊代价）为T(n)/n。
    以前的时间复杂度分析都是以单次操作为对象的分析它的最坏时间复杂度，而聚集分析所分析的是N次操作的总时间的最坏情况。应注意其与平均情况分析的不同之处！

  聚集分析：由序列的总最坏时间推导单次的平摊时间
- 记账法：对序列操作中的每一个操作收取一定的费用，当所收取的费用比它实际应支付的费用多时就把多余的部分当作存款存起来，一个操作的平摊代价可以看作两部分：实际代价和存款（或被储蓄或被用完）。

  - 存款可以用来在以后补偿那些其平摊代价低于其实际代价的操作。
  - 如果希望通过对平摊代价的分析来说明每次操作的最坏情况平均代价较小，则操作序列的总平摊代价就必须是该序列的总的实际代价的一个上界。因为平摊代价是最坏时的平均代价，这意味着：只能有存款而不能有欠账！
  - 明白这个存款不能为负很重要，因为这种分析方法是不允许欠账的，否则就不能满足平摊代价是操作总时间的最坏情况的平均这个定义。
  - Eg: 对于栈的操作，当每次入栈时记账支付2元，1元支付该PUSH操作的实际代价，还有1元用于支付该元素被POP出来时的代价。则可以保证在任何时间内都不会有欠账，POP操作可以不收取任何费用。

- 势能方法：将已预付的工作作为一种"势能"保存，它在需要时可以释放出来，以支付后面的操作。势能是与整个数据结构而不是其中的个别对象发生联系的。
    （记账法中的账与个别对象发生联系，比如栈操作时支付的2元就记在入栈的那个元素上）
- ``动态表：比如C++ STL中的vector``

  - 可以通过平摊分析证明：插入和删除操作的平摊代价都仅为O(1)
  - 表扩张时：常用的启发式（启发信息就是容器原来的大小）方法是分配一个原表两倍大小的新表。
  - 可以通过聚集分析来证明插入操作的平摊代价：
        :math:`\sum_{i=1}^N C_i \le N + \sum_{j=0}^{\llcorner lgN \lrcorner} 2^j < n + 2n =3n`

    所以每次操作的平摊代价为O(1)=3
  - 通过记账法来证明插入、删除操作的平摊代价：

    - 插入操作：每次插入操作需要支付3元的代价，分别是：将其自身插入到当前表中、当表扩张时其自身的移动、以及对另一个在扩张表时已经移动过的另一项的移动。于1+1+1=3元的代价

        +-------------------------------------------+-------------------------------------+
        | 原有的元素......                          | NewItem                             |
        +-------------------------------------------+-------------------------------------+
        | 在原有的元素里还要再支付一个可能移动的代价| 支付自身以后可能的移动操作的代价    |
        +-------------------------------------------+-------------------------------------+

    - 并且可以证明：无论扩张时分配多少倍的新空间，每次插入操作的平摊代价都大于2。所以标准库中取每次扩张时进行翻倍，这是一个均衡的很好的选择。
    - 删除操作：

      - C++STL的vector不会在删除元素时进行自动收缩。假设如果在元素不足1/2时进行收缩，最坏时会产生在这种情况（来回收缩和扩张）：
          在一次扩张之后，还没有做足够的删除来支付一次收缩的代价。类似地，在一次收缩之后，也有可能没有做足够的插入来支付一次扩张的代价。
          **所以C++ vector选择了在删除元素时不自动收缩来回避这个问题** 。如果要进行收缩，应该是元素减少为1/4时进行自动收缩，这样才能保证在表收缩之后有足够的存款进行支付一次收缩的代价。

      - 当删除至1/4个元素时进行自动收缩的平摊代价分析：
          每次删除操作需要支付的代价为：其自身的删除操作代价和剩下的元素可能进行的移动操作的代价。
          即最坏为：1 + 1 = 2元的代价，最好为1 + 0.33333 = 1.3333元的代价，取最坏情况为2元。

          +------------------------------+----------------------------+-----------------------------+----------------------------+
          | 剩下的元素......             |                            |                             |                            |
          +------------------------------+----------------------------+-----------------------------+----------------------------+
          | 还要再支付一个可能移动的代价 |                            | 最早可能从这开始删除        | 最晚可能从这开始删除       |
          +------------------------------+----------------------------+-----------------------------+----------------------------+

    - 因此： **对于动态表的插入和删除操作而言，每个操作的平摊代价都有一个常数的上界。**



第五部分：高级数据结构
==================================================
- B树是一种被设计成专门存储在磁盘上的平衡查找树。因为磁盘的速度远远慢于内存，所以B树被设计成尽量减少磁盘访问的次数，
  知道了这一点之后就会很清楚明白B树的变形B+树了，B＋树通过将数据存储在叶子结点从而增大了一个结点所包含的信息进而更加的减少了磁盘的访问次数。
- 可合并堆：这种堆支持Insert,Mininum,Extract-Min,union,delete,decrease-key操作。
- 二项堆能够在O(lgn)的最坏情况时间内支持以上的各种操作。当必须支持union操作时，二项堆优越于二叉堆，因为后者在最坏情况下，合并两个二叉堆要花O(n)的时间。
- 斐波那契堆对于以上各种除了extract-min,delete的操作外都只需要O(1)的实际时间，而extract-min,delete也只需要O(lgn)的平摊时间。它的重要优点在于decrease-key也只需要O(1)的平摊时间。
  注意斐波那契堆的一些操作都只是平摊时间，并非最坏情况时间。现代的快速图算法中，很多是使用斐波那契堆作为其核心数据结构的。
- 不相交集合（查并集）：通过用一棵简单的有根树来表示每个集合，就可以得到惊人的快速操作：一个由m个操作构成的序列的运行时间为O(n α(n) )，而对于宇宙中的原子数总和n，α(n)也<=4，所以可以认为实际时间是O(n)。



第18章：B树
--------------------------------------------------
- B树与红黑树类似，但是在 **降低磁盘的I/O次数** 方面要更好一些。所以B树一般都是应用于磁盘操作的系统中。
- B树思想产生在背景：就是大规模数据存储中，实现索引查询这样一个实际背景下，树节点存储的元素数量是有限的（如果元素数量非常多的话，查找就退化成节点内部的线性查找了），这样导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下（因为读取一次磁盘相当于访问了无数次内存！），那么如何减少树的深度（当然是不能减少查询的数据量），一个基本的、很自然的想法就是：采用多叉树结构（由于树节点元素数量是有限的，自然该节点的子树数量也就是有限的） [5]_
- 在大多数系统中，B树算法的运行时间主要由它所执行的disk-read和disk-write操作的次数所决定，因而应该有效地使用这两种操作，即让它们读取更多的信息更少的次数。
  由于这个原因，在B树中，一个结点的大小通常相当于一个完整的磁盘页。因此，一个B树结点可以拥有的子女数就由磁盘页的大小所决定。
- 很明显：B树的分支因子越大越好，因为这样运行时间的绝大部分都是由磁盘存取次数决定的。分支因子越大，需要进行的磁盘存取次数就越少。
  但是这个分支因子是有限制的，一个结点的总大小不能大于磁盘中一个页的大小，否则在一个结点内操作时还要来回访问内存，反而会拖慢效率。
- 一个常见的B树的变形，称作为B＋树，所有的附属数据都保存在叶结点中，只将关键字和子女指针保存于内结点里，因此最大化了内结点的分支因子。
- B 树又叫平衡多路查找树。一棵t（t>=2，t是B树的最小度数）阶的B 树的特性如下：

  .. image:: images/btree.jpg

  - 每个叶子结点具有相同的深度，即树的高度h。
  - 每一个结点能包含的关键字数有一个上界和下界，这些界可用B树的最小度数t来表示（t-1 <= n <= 2t-1）：

    - 每个非根的结点必须至少含有t-1个关键字。每个非根的内结点至少有t个子女。如果树是非空的，则根结点至少包含一个关键字（同时意味着2个子女，根结点的下限永远是1个关键字，2个女子）；
    - 每个结点可包含至多2t-1个关键字。所以一个内结点至多可有2t个子女。如果一个结点恰好有2t-1个关键字，我们就说这个结点是满的；

  - 每个非终端结点中包含有n个关键字信息： (n，P0，K1，P1，K2，P2，......，Kn，Pn)。其中：

    - Ki (i=1...n)为关键字，且关键字按顺序升序排序K(i-1)< Ki。
    - Pi为指向子树根的接点，且指针P(i-1)指向子树种所有结点的关键字均小于Ki，但都大于K(i-1)。
    - 关键字的个数n必须满足：t-1 <= n <= 2t-1。

- 当t=2时为最简单的B树，又称为2-3-4树，即每个结点的孩子数可能为2,3,4个；包含的关键字个数为1,2,3个。
- 与红黑树相比，这里我们看到了B树的能力，虽然两者的高度都以O(lgn)的速度增长，但对于B树来说底要大很多倍。对大多数的树的操作来说，要查找的结点数在B树中要比红黑树中少大约lgt的因子。因为在树中查找任意一个结点通常需要一次磁盘存取，所以磁盘存取的次数大大的减少了。
    如果让我来实现一个数据库系统，我肯定也会选取类似B树的数据结构作为基本。

- B树首先一是棵查找树，所以它的查找操作将会非常简单和高效。
- **B树的插入操作** ：插入操作一定发生在叶子结点，因为B树首先必须是一棵查找树。

  这是我的代码所使用的思路：因为插入操作肯定是在叶子结点上进行的,首先顺着书向下走直到要进行插入操作的叶子结点将新值插入到该叶子结点中去. 如果因为这个插入操作而使用该结点的值的个数>2*t-1的上界，就需要递归向上进行分裂操作。如果分裂到了根结点，还要处理树长高的情况。这种思路简单，但不是很高效，因为经过了两趟，一趟向下；一趟向上。

  《算法导论》上介绍的方法则比较好：边下行边分裂，当沿着树往下查找新关键字时所属位置时，就分裂沿途遇到的每个满结点（包含叶结点本身）。因此每当要分裂一个满结点时，就能确保它的双亲不是满的。这种方法只需要一趟就可以完成，极大的减少磁盘的读取次数，而这正是B树所设计追求的，因而这种一趟的方法是更优的！
  
  但是我的代码只实现了我自己的简单方法，书上的方法实现起来应该也不难。
- 插入操作时B树的分裂是B树升高的唯一途径！
- **B树的删除操作** ：删除操作稍微复杂一些，因为删除操作不仅仅会发生在叶子结点，还可能会发生在内结点，这与插入操作不同。但是可以通过一个技巧消除这一点，找到要删除结点的前驱，然后与要删除的结点的关键值进行对调，再删除这个前驱结点就可以保证每次要删除的都是叶子结点。

  同样有可以进行边下行边合并的快速方法，而同样，我这里的代码也没有实现这种快速的方法，而是选用了传统的两趟的删除方法，具体如下：

  任一关键字K的中序前趋(后继)必是K的左子树(右子树)中最右(左)下的结点中最后(最前)一个关键字。根据B树的性质：B树上每一个结点的Key的个数必须为[t-1, 2*t-1]之间，所以这里的Min = t - 1。若被删关键字K所在的结点非树叶，则用K的中序前趋(或后继)K'取代K，然后从叶子中删去K'。从叶子*x开始删去某关键字K的三种情形为：

  - 情形一：若x->keynum>Min，则只需删去K及其右指针(\*x是叶子，K的右指针为空)即可使删除操作结束。
  - 情形二：若x->keynum=Min，该叶子中的关键字个数已是最小值，删K及其右指针后会破坏B-树的性质(3)。若*x的左(或右)邻兄弟结点*y中的关键字数目大于Min，则将*y中的最大(或最小)关键字上移至双亲结点*parent中，而将*parent中相应的关键字下移至x中。显然这种移动使得双亲中关键字数目不变；\*y被移出一个关键字，故其keynum减1，因它原大于Min，故减少1个关键字后keynum仍大于等于Min；而*x中已移入一个关键字，故删K后*x中仍有Min个关键字。涉及移动关键字的三个结点均满足B-树的性质(3)。 请读者验证，上述操作后仍满足B-树的性质(1)。移动完成后，删除过程亦结束。
  - 情形三：若*x及其相邻的左右兄弟(也可能只有一个兄弟)中的关键字数目均为最小值Min，则上述的移动操作就不奏效，此时须*x和左或右兄弟合并。不妨设*x有右邻兄弟*y(对左邻兄弟的讨论与此类似)，在*x中删去K及其右子树后，将双亲结点*parent中介于*x和*y之间的关键字K，作为中间关键字，与并x和*y中的关键字一起"合并"为一个新的结点取代*x和*y。因为*x和*y原各有Min个关键字，从双亲中移人的K'抵消了从*x中删除的K，故新结点中恰有2Min(即2「m/2」-2≤m-1)个关键字，没有破坏B-树的性质(3)。但由于K'从双亲中移到新结点后，相当于从*parent中删去了K'，若parent->keynum原大于Min，则删除操作到此结束；否则，同样要通过移动*parent的左右兄弟中的关键字或将*parent与其 左右兄弟合并的方法来维护B-树性质。最坏情况下，合并操作会向上传播至根，当根中只有一个关键字时，合并操作将会使根结点及其两个孩子合并成一个新的根，从而使整棵树的高度减少一层。

- 删除操作看似复杂，但是对一棵高度为h的B树，它只需要O(h)次磁盘操作，因为在递归调用的过程之间，仅需要O(1)次对disk-read,disk-write的调用，时间复杂度为O(th)=O(tlogtn).

.. literalinclude:: source/B_tree.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/B_tree.output

.. image:: images/B_tree_output1.png

.. image:: images/B_tree_output2.png

.. image:: images/B_tree_output4.png
    :scale: 60%



第19章：二项堆
--------------------------------------------------
- 可合并堆就是普通的堆+支持decrease-key,union操作，但是应该高效的实现union操作是可合并堆最关键的部分。
- 如果不需要高效的支持UNION操作，则普通的堆结构就很好了
- 对于 **所有的堆结构：二叉堆、二项堆、斐波那契堆。它们的search操作都是很慢的** ，不能高效的支持search操作！
    因而在decrease-key和delete等涉及结点的操作时都需要一个指向结点的指针。

- 一个二项\ **堆**\ 是由一\ **组**\ 二项\ **树**\ 所构成的。
- 二项树是一种递归的定义:

  - 二项树B[0]仅仅包含一个节点
  - B[k]是由两棵B[k-1]二项树组成，其中一颗树是另外一颗树的子树。

  下面是B0 - B4二项树：

  .. image:: images/binary_tree1.png
 
- 显然二项树具有如下的性质：

  - 对于树B[k]该树含有2^k个节点；
  - 树的高度是k；
  - 在深度为i中含有Cik节点，其中i = 0， 1，2 ... , k;

- 二项堆是由一组满足下面的二项树组成：

  #. H中的每个二项树遵循最小堆性质：结点的关键字大于或等于其父结点的关键字。我们说这种树是最小堆有序的。
  #. 对于任意的整数k的话，在H不存在另外一个度数也是k的二项树；即至多（有0或1棵）有一棵二项树的根具有度数K。
  
  第一个性质保证了二项树的根结点包含了最小的关键字。第二个性质则说明结点数为n的二项堆最多只有logn + 1棵二项树。

- 就像二进制的0、1可以表示出任意的数值，任意个结点数的堆也可以由二项堆来表示。
    例如：13<1101>个结点的堆可由二项树B3,B2,B0来表示出来。

- 根表：一个二项堆中的各二项树的根被组织成一个键表，我们称之为根表！

  .. image:: images/binary_tree2.png

- 二项堆的最重要的一个操作就是UNION操作，其它的操作都可以在UNION操作的基础上轻松的实现。

  大概思路为：将两个二项堆的根表连接起来组成一个大的二项树的连接，按"度"的单调递增顺序进行排序之后，从左至右来消除具有重复度的二项树。因为原本的每个二项堆中任意度K至多只有一个相应的二项树，所以这个消除重复的操作会非常容易。从小到大找到两个相同度K的二项树，然后连接成一个K+1度的二项树，直到链尾时合并完毕。
  
  此时的二项堆就满足对任意度K至多只有一棵二项树。此操作的时间复杂度为O(logn)

  .. image:: images/binary_tree3.png

- **其它的操作都可以通过将UNION操作作为基础操作来实现** ：

  - 插入：NEW一个一个结点的二项堆，然后调用UNION与原二项堆来进行合并
  - 抽取最小关键字的结点：

    - 从原二项堆的根键上取下最小关键字结点的二项树，独立出来；
    - 将独立出来的二项树的根结点去掉（这就是整个二项堆的最小关键字），然后将它的子结点逆转并组合成为一个不含最小值的二项堆；
    - 最后将这个新的二项堆与原来剩下的二项堆进行合并操作。

    .. image:: images/binary_tree4.jpg
  
  - 减少关键字：将要减小的关键字与它的父结点进行比较，如果需要就进行交换操作直到根链。使得该关键字在堆中"冒泡上升"直到正确的位置。
  - 删除一个关键字（调用其它操作实现）::

      decrease-key(x, -max)
      extract-min()

.. literalinclude:: source/binomial_heaps.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/binomial_heaps.output

.. image:: images/binomial_heaps_output1.png

.. image:: images/binomial_heaps_output2.png

.. image:: images/binomial_heaps_output3.png
    :scale: 80%

.. image:: images/binomial_heaps_output4.png



第20章：斐波那契堆
--------------------------------------------------
- 斐波那契堆与二项堆相比，取消掉了2个限制，因而更加的松散了：

  - 斐波那契堆中的树不必要求是二项树
  - 根链中的树必没有"每个度数至多只能有一棵树"的要求了

- 斐波那契堆首先也是一种堆，所以它也满足堆的性质：子结点比父结点大！
- 斐波那契堆的特点：不涉及删除元素的操作有O(1)的平摊时间。 Extract-Min和Delete的数目和其它相比较小时效率更佳。
- 斐波那契堆在优化加速图算法中有很大的用途。比如用于解决诸如最小生成树、寻找单源最短路径等问题的快速算法都要用到斐波那契堆。
- 斐波那契堆由一组树（并不要求二项树）组成。但实际上，这种堆松散地基于二项堆。斐波那契堆的结构比二项堆更松散一些，从而可以改进渐近时间界。对结构的维护工作尽可能的拖延，直到方便时再做。
- 同任何堆一样：斐波那契堆也不能很好地支持search操作。
- 与二项堆中的树都是有序的不同，斐波那契堆中的树都是有根而无序的。每个节点包含一个指向其父节点的指针p[x]，以及一个指向其任一子女的指针child[x]（指向子女双链表），因为这些子结点本身就没有顺序关系（兄弟之间是无序的，所以更加松散），所以可以随意的指向。

  .. image:: images/fibonacci_heap.jpg 

  其中：mark[x]用于指示自从x上一次成为另一个结点子女以来，它是否失掉了一个孩子，在图中用黑色的结点来表示。
- 如果仅支持"make-heap, insert, minimum, extract-min, union操作，那么每个斐波那契堆就只是一组"无序的"二项树。
- 对斐波那契堆上的各种可合并堆操作来说，其关键思想就是尽可能久地将工作推后。
    这种lazy-compute很可能会大大的改善性质，但是也会造成后面真正进行维护时的算法执行时间的常数因子过大。

- 斐波那契堆是通过指向根链上的最小结点的指针min[H]来进行访问的。
- 斐波那契堆的日常操作都非常简单，除了extract-min以外：
    注：这里的时间复杂度均指的是平摊代价，而非一般使用的最坏情况时间。

  - 插入操作O(1)：直接插入到根链上去，再与最小结点的min[H]进行一下比如，如果比min[H]还小，就将min[H]指针指向新插入的结点。
  - 寻找最小节点O(1)： min[x]指向的节点即为最小节点。
  - 合并两个斐波那契堆O(1)：分为3步： 1：合并根表；2：设置新的min[h]；3。重置n[x]。
    斐波那契堆过于松散的性质使得连合并操作都只需要O(1)的时间，只需要合并两个根链就可以了。
  - 抽取最小节点O(lgn)：相对来说这是最复杂的工作。被延迟的对根表的调整工作最终由这个操作进行。
    
    | 1：去掉根表上的最小值，将这个最小值的每个孩子都加入根表；2：将根表上相同度数树的合并。
    | （一点都不复杂，代码与二项堆一样容易实现）
  
    这里的在根链上合并操作与二项树基本相同，就是按"度的递增"顺序排列所有的子树之后，合并具有相同度的子树，使得最后的根链上每一个度K都只有至多一棵子树。（这里又与二项堆神似了，所以说斐波那契堆是松散地基于二项堆）。形式化地描述为：
      "调整根表的步骤 1：在根表中找出两个具有相同度数的根x和y，且key[x]<key[y] 2：将y与x连接。将y从根表里去掉，成为x一个孩子，并增加degree[x]。同时，如果y上有标记的话也被清除掉。"

  - 减小一个节点的权值O(1)：
    
    - 若此减小不影响堆序，不作调整； 
    - 若影响堆序，则从堆中删除该节点，将其加入根表，并检查其父亲的mark位；若为false，则停止，并将其置为true；若为true，则删除其父亲，继续递归向上执行；直到一个节点mark域为false或该节点为根节点为止。

  - 删除一个节点O(lgn)：1：将该节点权值调整至最小；2：抽取最小值。

- 总结：斐波那契堆之所以高效，就是因为它松散了，将很多二项堆要维护的工作都推迟到了extract-min和delete操作之中去了。而它之所以可以这样进行偷懒，就是因为它的限制非常的少，并没有像二项堆一样的要求子树全部必须是二项树，而且根表上的任意度数的子树至多只能有一个。没有了这些限制，斐波那契堆就可以实现的非常的松散以至少将日常工作的维护时间都平摊给extract-min和delete操作。

  但是，正如斐波那契堆现在更多的应用于理论中，实践中使用的并不多的现象所映射的，在大部分的应用中斐波那契堆并不能带来大幅度的效率的提升。因为它并不是减少了要做的工作，只是将很多要做的工作都进行了推迟，可能出现这样的情况：

  - 合并操作O(lgn)=>O(1)
  - 插入操作O(lgn)=>O(1)
  - 减少权值操作O(lgn)=>O(1)
  - 但是extract-min操作O(lgn)=>O(3*lgn)≈O(lgn)

  这使得在理论分析时，它的所有操作的渐近时间都至少不变坏，但是在实际使用中却没有任何的改善。
  当然这是最坏的情况了，一般对于规模足够大的输入，斐波那契堆是能够很大改善算法的性能的，尤其是对于一些extract-min和delete操作少的情况。比如斐波那契堆的使用能加快 Prime 和 Djikstra 算法的执行速度。

.. literalinclude:: source/fibonacci_heap.h
    :language: cpp
    :linenos:

.. literalinclude:: source/fibonacci_heap.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/fibonacci_heap.output



第21章：用于不相交集合的数据结构
--------------------------------------------------
- 用于不相交集合的数据结构又称为 **查并集** ：在很多的应用中（比如图的算法中经常使用，还有哈夫曼编码等），将N个不同的元素分成一组不相交的集合，不相交集合上有两个重要的操作：
  
  - **找出给定元素所属的集合**
  - **合并两个集合**

- 常用的两种表示方法：一种为链表的实现；另一种是更有效的树的表示法。
- 采用树的表示法的运行时间在实践上是线性的，但从理论上来说是超线性的。
    这句话的意思是：理论上分析的运行时间是大于（超过）线性的，但是在实践中证明它的运行时间却是线性的。

- 每个集合通过一个代表来识别，代表即集合中的某个成员。在某些应用中，哪一个成员被选作代表是无所谓的，而关键的是在集合未被修改的前提下，两次寻找得到的答案应该是相同的。
- 不相交集合的链表表示：

  .. image:: images/cbj1.jpg

  - 每个链表的第一个对象作为它所在集合的代表
  - 链表上的第个结点都有指向它所在集合的代表的指针，从而使得找出所属集合的操作时间降为O(1)
  - 但是因为每个结点都有指向代表的指针，使得在进行合并操作时，需要更新所有结点的指向代表的指针，从而时间复杂度为O(n)。这算是有利也有弊吧。
  - 一种加权合并启发式策略：在进行合并操作时，总是把较短的表拼到较长的表上去。
      这种简单的小技巧的正式的名字都可以叫做：启发式信息，启发式策略！以后涉及到这种情况的时候也可以这么叫。

- 不相交集合森林表示法：用有根树来表示集合，树中的每个结点都包含集合的一个成员，每棵树表示一个集合。每个成员仅指向其父结点，每棵树的根包含了代表，并且是它自己的父结点。

  .. image:: images/cbj2.png

- 在引入了两种启发式策略（按秩合并、路径压缩）之后，不相交集合森林表示法就是目前已经的、渐近意义最快的不相交集合数据结构了。

  - 接秩合并：便包含较少结点的树的根指向包含较多结点的树的根。秩表示的是结点高度的一个上界。
    每个结点的秩是结点高度的一个上界，只有在进行UNION操作时，而且进行合并操作的两个集合的秩相同时，才会给最后的根结点的秩+1。代码如下::

      template<typename T>
      static void _Link(DisjointSet<T> &x, DisjointSet<T> &y) {
        if (x.Rank > y.Rank) {
          y.Parent = &x;
        }
        else {
          x.Parent = &y;
        }
        if (x.Rank == y.Rank) {
          //只有在秩相同时才会将最后的根结点的秩+1
          ++y.Rank;
        }
      }

  - 路径压缩：（查找路径：在查找一个结点所属的集合时的查找路径上访问过的所有的结点）在每次进行查找元素所属集合的操作时，使得查找路径上的每个结点都直接指向根结点。路径压缩并不改变结点的秩。
    这种路径压缩使用了两趟的方法：一趟是沿查找路径向上升，直至找到根；第二趟时沿查找路径下降，以便更新查找路径上的每个结点，使之指向根。这种路径压缩的思想简直是太棒了！神奇啊！应该学习这种思想，碰到类似的问题也要想到类似的解决办法

      +------------------------------+---------------------------------------------------------------+
      |                              | 代码如下::                                                    |
      |  .. image:: images/cbj3.jpg  |                                                               |
      |                              |   template<typename T>                                        |
      |                              |   static DisjointSet<T> & FindSet(DisjointSet<T> &a_set) {    |
      |                              |     //路径压缩                                                |
      |                              |     if (&a_set != a_set.Parent) {                             |
      |                              |       //判断本身不是代表                                      |
      |                              |       a_set.Parent = &FindSet(*a_set.Parent);                 |
      |                              |     }                                                         |
      |                              |     return *a_set.Parent;                                     |
      |                              |   }*                                                          |
      |                              |                                                               |
      +------------------------------+---------------------------------------------------------------+

  - 使用了这两种启发式策略之后，算法的最坏运行时间为O(m*alpha(n))，其中m是操作序列中所有的操作的个数总和。
    对于宇宙中的所有原子数之和x, alpha(x)<=4，所以说算法的实际时间是线性的。

.. literalinclude:: source/disjoint_set_forest.h
    :language: cpp
    :linenos:

.. literalinclude:: source/disjoint_set_forest.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/disjoint_set_forest.output



第六部分：图算法
==================================================

- 图是计算机科学中常用的一类数据结构。
- 两种图的遍历方法：广度优先遍历（求每条边都是单位权值的图的最短路径）和深度优先遍历（拓扑排序、将有向图分解为强连通子图）。
- 最小权生成树：由连接了图中所有顶点的、且权值最小的路径所构成。
- 有向图中的最大流问题：这是个一般性的问题，会以多种形式出现；一个好的计算最大流量的算法可以用来有效地解决多种相关的问题。


第22章：图的基本算法
--------------------------------------------------
- 图的搜索技术是图算法领域的核心
- 两种最普通的图的表示方法：邻接表法（节省空间）和邻接矩阵法（查询高效）
    稀疏图多用邻接表法来表示；而稠密图则用邻接矩阵法来表示比较好。

- 不论是有向图还是无向图，邻接表法都有一个很好的特性，即它所需要的存储空间为O(V+E)；邻接表表示法稍作修改就能支持其它多种图的变体，因而有着很强的适应性。
- 无向图的邻接矩阵A就是它的转置矩阵：A=A\ :sup:`T` 。在某些应用中，可以只存储邻接矩阵的对角线及对角线以上的部分，这样一来，图所占用的存储空间几乎可以减少一半
- 邻接表表示和邻接矩阵表示在渐近意义上至少是一样有效的，但由于邻接矩阵简单明了，因而当图较小时，更多多地采用邻接矩阵来表示。另外，如果一个图不是加权的，采用邻接矩阵的存储形式还有一个优越性：在存储邻接矩阵的每个元素时，可以只用一个二进位，而不必有一个字的空间。
  
  这样，当采用了二进位以及表示无向图的技巧时，邻接矩阵法的占用空间大的缺点就可以得到一定程度上的改善！
- 广度优先搜索能够得到这种意义上的最短路径：每条边的权值都为1，即所有的边都具有单位权值。
    广度优先所产生的广度优先树是每个顶点到s的最短距离。
    
- 深度优先搜索除了创建一个深度优先森林外，深度优先搜索同时为每个顶点加盖时间戳。每个顶点v有两个时间戳：当顶点v第一次被发现时，记录下第一个时间戳d[v]，当结束检查v的邻接表时，记录下第二个时间戳f[v]。
    许多基于深度优先搜索的图算法都用到了时间戳，它们对推算深度优先搜索的时行情况有很大的帮助。

- 这次重新复习深度优先算法，得到的最大的启示就是使用了这2个时间戳，真的是很有用很好的创新啊。当记录下这2个时间戳之后，很多东西都可以由这对时间戳来推导出来了（比如拓扑排序、深度遍历的次序等）。
- 广度搜索通常用于从某个源顶点开始，寻找最短路径距离（以及相关的先辈子图）。深度优先搜索通常作为另一个算法中的一个子程序。
- 后裔区间的嵌套：在一个（有向或无向）图G中的深度优先森林中，顶点v是顶点u的后裔，当且仅当d[u]<d[v]<f[v]<d[u]，由这关系可以推导出大部分的与时间戳相关的性质。
- 边的分类：树边、反向边、正向边、交叉边。
- 拓扑排序：在很多应用中，有向无回路图说明事件发生的先后次序。
    在深度优先遍历的基础上，对有向无回路图进行拓扑排序简直是小菜一蹀。根据遍历所得到的时间戳f[i]逆向排序就好了。

- 拓扑排序的顶点以与其完成时间时间相反的顺序出现。这种新方法真是长见识啊，比我以前使用的方法好多了，这种方法在遍历完只需要一个简单的sort就完成了拓扑排序，时间复杂度也降低为O(V+E)。
- 这种新的拓扑排序的方法的理论基础是：对于任一对不同的顶点u,v，如果存在一条从u -> v的边，那么u在拓扑排序的结果中一定在v的前面。而又根据后裔区间嵌套的定理：如果存在u -> v，那么f[v]<f[u]，所以得证根据f逆向排序得到的顺序一定为拓扑排序。
- 强连通分支：有向图G=(V, E)的一个强连通分支就是一个最大的顶点集合C，对于C中的每一对顶点u,v，都有u -> v及v -> u；亦即顶点u和v是互相可达的。
- 寻找强连通分支的简明算法：

  .. image:: images/graphic_base.jpg
 
  - 对G进行深度优先遍历得到每个顶点的时间戳f[x]；
  - 求得G的返回图GT；
  - 按照f[x]的逆向顺序为顶点顺序对GT进行深度优先遍历，即按照G的拓扑排序的顺序对GT再进行深度优先遍历；
  - 步骤c得到的各棵子树就是原图G的各强连通分支。

- 寻找强连通分支的算法的时间复杂度为O(V+E)。

.. literalinclude:: source/deapth_first_search.h
    :language: cpp
    :linenos:

.. literalinclude:: source/deapth_first_search.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/deapth_first_search.output

.. literalinclude:: source/breadth_first_search.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/breadth_first_search.output

.. literalinclude:: source/topological_sort.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/topological_sort.output

.. literalinclude:: source/strongly_connected_component.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/strongly_connected_component.output



第23章：最小生成树
--------------------------------------------------
- 最小（权值）生成树：由n-1条边，连接了所有的n个顶点，并且所有边上的权值和最小。
- Kruskal算法和Prim算法：这两种算法中都使用普通的二叉堆就可以很容易地达到O(ElgV)的运行时间。通过采用斐波那契堆，Prim算法的运行时间可以减少到O(E +VlgV)，这对于稠密图来说是个很大的改进。
- 贪心策略可以在最小生成树问题中得到最优解，事实上这里的Kruskal、Prim方法都是贪心算法。它们也都是可以被证明的一定能够得到最优解！
- 在Kruskal算法中，集合A是一个森林，加入集合A中的安全边总是图中连接两个不同连通分支的最小权边；

  .. image:: images/min_generate_tree1.png

  在Prim算法中，集合A仅形成单棵树，添入集合A中的安全边总是连接树与一个不在树中的顶点的最小权边。

  .. image:: images/min_generate_tree2.png
 
- 为什么说Prim算法有着更好的实际效率：

  - Prim算法在执行的过程中，将不在树中的所有顶点都放在一个基于key域的最小优先队列Q中；
  - 每次在选取安全边时，只需要从Q中弹出最小key值的顶点即可，而不需要像Kruskal一样为对所有的边的权值进行一次排序；
  - Prim算法使用了用于快速或者最小权值边的技巧（二叉堆、二项堆、斐波那契堆）来加速算法的运行。在最朴素的选取安全边的算法中，需要遍历剩下的所有的边，所需要的时间复杂度为O(EV)，而采用堆来优化后只需要O(ElgV)，大大地改善了算法的执行时间（堆的好处）。
  - 不过Kruskal算法实现简单，所以对于一般的应用Kruskal算法很常见。

- Prim算法的性能取决于优先队列Q是如何实现的，因此如果使用斐波那契堆来实现最小优先队列，就可以将Prim算法的运行时间改进为O(E+VlgV)。

.. literalinclude:: source/kruskal.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/kruskal.output

.. literalinclude:: source/prim.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/prim.output


第24章：单源最短路径
--------------------------------------------------
- 最短路径：一个顶点到另一个顶点的最短权值路径。广度优先搜索算法就是一种在无权（单位权值）图上执行的最短路径算法。
- 从渐近意义上来说，解决一对顶点的最短路径问题的复杂度与单源最短路径的复杂度相同。
- 最短路径算法通常依赖于一种性质，也就是一条两顶点间的最短路径包含路径上其它的最短路径。这里动态规划和贪心算法的特征之一：最优子结构。
    Dijkstra算法是一个贪心算法。DIJKSTRA算法假定输入图中的所有边的权值都是非负的，而floyd算法允许输入边存在负权边，只要不存在从源点可达的负权回路。而且如果存在着负权回路，它还能检测出来。

- Bellman-Ford算法非常简单：对所有的边进行|v|-1遍循环，在每次循环中对每一条边进行松弛的操作。

  .. image:: images/single_min_road1.jpg

- 按顶点的拓扑顺序对某加权有向无回路图的边进行松弛后，就可以在O(V+E)时间内计算出单源最短路径。在一个有向无回路图中最短路径总是存在的，因为即使图中有权值为负的边，也不可能存在负权回路（因为它根本没有任何回路）。
- Dijkstra算法是一种贪心策略的算法，它的运行时间一般比Bellman-Ford算法要好::

    DIJKSTRA(G, w, s)
    1  INITIALIZE-SINGLE-SOURCE(G, s)
    2  S ← Ø
    3  Q ← V[G]
    4  while Q ≠ Ø
    5      do u ← EXTRACT-MIN(Q)
    6         S ← S ∪{u}
    7         for each vertex v ∈ Adj[u]
    8             do RELAX(u, v, w)

  .. image:: images/single_min_road2.jpg

  Dijkstra算法在每次循环中，每次仅仅提取d值最小的顶点u是保证这个贪心策略正确性的关键核心所在。因为通过集合S中所有的元素都已经去试着松弛过u了，而非S中的点由于本身它的d值都比u要大，所以即使用它们中的任何一个去松弛u，也不可能达到比现在更小的d值了。因此，在每次循环中选取当前d值最小的顶点加入到S集合中去一定能保证最后得到全局最优解。
- 很多问题都可以转换成图的问题，使用最短路径的算法来加以解决的。 **要善于把一些看似不相干的问题转化为图的问题** 

.. literalinclude:: source/bellman_ford.h
    :language: cpp
    :linenos:

.. literalinclude:: source/bellman_ford.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/bellman_ford.output

.. literalinclude:: source/dijkstra.h
    :language: cpp
    :linenos:

.. literalinclude:: source/dijkstra.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/dijkstra.output



第25章：每对顶点间的最短路径
--------------------------------------------------
- Folyd-Warshall是一个动态规划算法，运行时间为O(V^3)；Johnson算法使用了几种算法作为其子算法，运行时间为O(V^2 lgV+VE)，尤其适合对大型稀疏图。
- 求每对顶点之间的最短路径问题很适合于动态规划算法来求解，因为它满足"最优子问题的结构"和"重叠子问题"两个特征。
- 一个朴素的思路就是使用Lij(m)来表示从顶点i到顶点j的，中间包含至多m条路径的最短路径。这种思路是朴素的动态规划思想，效率比较好，时间复杂度为O(V3lgV)，但是后面提及的两种算法都对这个"最优子问题的结构"进行了更多的优化。
- Folyd-Warshall算法的运行时间为O(V3)，它允许权值为负的边，但是假定了不存在权值为负的回路。而且它的代码是紧凑的，而且不包含复杂的数据结构，隐含的常数因子很小。因此，即便对于中等规模的输入图来说Folyd-Warshall算法仍然相当的实用。
- Folyd-Warshall的核心在于：它改进了"最优子问题结构"，使用dij(k)来表示从顶点i到顶点j、且满足所有中间顶点皆属于集合{1,2,…,k}的一条最短路径的权值。这种限定了起始点的技巧大大的减少了实现的计算量。
- 有向图的传递闭包：G的传递闭包定义为图G*=(V, E*)，其中E*={(i,j) : 图G中存在着一条从i到j的路径}。

  对于一个大图，即使是只需要确定是否存在路径可达都不是一件很容易的事件。解决此问题的整体思路与Folyd-Warshall算法一样，只是把Folyd-Warshall算法中的min和+操作替换为相应的OR和AND逻辑运算来加快速度，本质并没有区别。
- Johnson算法可在O(V2lgV+VE)时间内，求出每对顶点间的最短路径。Johnson算法使用Dijkstra和Bellman-Ford算法作为其子程序。
  
  Johnson算法是一个实际上非常好的算法，它使得所有的情况（可能存在负权值和负权回路）都可以使用最好的Dijkstra算法来达到最好的运行效率。
- Johnson算法在所有的边为非负时，把每对顶点依次作为源点来执行Dijkstra算法，就可以找到每对顶点间的最短路径；利用斐波那契最小优先队列，该算法的运行时间为O(V2lgV+VE)。

  因此也可以总结出：对于确定无负权值的图，直接循环调用Dijkstra算法就是求每对顶点间最短路径的最佳算法。
- Johnson算法使用了重赋权技术，对每一条边的权值w赋予一个新的权值w’，使用新的边权值集合满足以下两个性质：

  - 对所有的顶点u,v，如果路径p是在权值函数w下从u到v的最短路径，当且仅当p也是在权值函数w’下从u到v的最短路径；
  - 对于所有的边u,v，新的权值w’(u,v)是非负的（于是满足Dijkstra算法的要求）。

- Johnson算法的简明步骤：

  .. image:: images/double_min_road.jpg
 
  - 生成一个新图G’，G’是在G上扩展一个起始点后的结果；
  - 在G’上调用Bellman-Ford算法。由于Bellman-Ford算法能够检测负权回路，如果存在负权回路则报告存在负权回路并结束整个算法；否则得到在G’上调用Bellman-Ford算法得到的h(x)函数；
  - 根据h(x)函数对G中的每一条边进行重赋权，使得G中的每一条边都是非负的；
  - 对重赋权后的G进行循环调用Dijkstra算法，得到每对顶点间的最短路径；
  - 对得到的每对顶点间的最短路径再根据h(x)函数反向构造出原来权值下的最短路径值。

.. literalinclude:: source/floyd_warshall.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/floyd_warshall.output

.. literalinclude:: source/johnson.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/johnson.output


第26章：最大流
--------------------------------------------------
- 流守恒：物质进入到某顶点的速度必须等于离开该顶点的速度。
- 最大流问题：是关于流网络的最简单问题，它提出这样的问题：在不违背容量限制的条件下，把物质从源点传输到汇点的最大速率是多少？
- 某个顶点处的总的净流量：为离开该顶点的总的正流量，减去进入该顶点的总的正流量。流守恒性的一种解释是这要瓣，即进行某个非源点非汇点顶点的正网络流，必须等于离开该顶点的正网络流。这个性质（即一个顶点处总的净流量必定为0）通常被非形式化地称为"流进等于流出"。
- 抵消：利用了抵消处理，可以将两城市间的运输用一个流来表示，该流在两个顶点之间的至多一条边上是正的。也就是说，任何在两城市间相互运输球的情况，都可以通过抵消将其转化成一个相等的情况，球只在一个方向上传输：沿正网络流的方向。

  这样：两个顶点之间至多有两条边，而这两条边至多会有一条有正的流值，另一条相应的边的流值为0（明白这点在理解算法时是有用的）。

  .. image:: images/max_flow1.jpg

- 具有多个源点和多个汇点的网络最大流问题可以转化成普通的单源点、单汇点的最大流问题（通过添加一个单一的源点和一个单一的汇点并置新添加的边的容量为无穷大来达到）。
 
- Ford-Fulkerson算法是求最大流的一般方法，它利用了三点：残留网络、增广路径、最大流最小割定理。

  - 残留网络：Gf由可以容纳更多网络流的边所组成；
  - 增广路径：为残留网络Gf上从s到t的一条简单路径p，其中p中所的边的最小权值为该增广路径的残留容量；
  - 最大流最小割定理：证明了Ford-Fulkerson算法能够得到全局最优解"当一个流是最大流，当且仅当它的残留网络不包含增广路径"。

- Ford-Fulkerson算法的简明步骤：
  
  .. image:: images/max_flow2.jpg
 
  - 初始化G中所有边的流为0；
  - 计算当前图与当前流所映射的残留网络Gf；
  - 从残留网络上选取一条增广路径并计算出残留容量，将残留容量添加到图的当前流上；
  - 循环步骤b-c直到残留网络Gf中不存在增广路径为止；
  - 此时的流即为G的最大流。

- 使用"广度优先搜索"来求增广路径的Ford-Fulkerson算法即称之为Edmonds-Karp算法，这种使用广度优先搜索来求增广路径的算法能够改善Ford-Fulkerson算法的运行时间。
- 最大二分匹配问题：这是最大流最重要的应用之一，并且有许多其它的问题可以归约成它，有着许多的实际应用。
 
  .. image:: images/max_flow3.jpg

  例如：把一个机器集合L和要同时执行的任何集合R相匹配。E中的边(u,v)，就说明一台特定机器u能够完成一项特定任务v，最大匹配可以为尽可能多的机器提供任务）
- 解决最大二分匹配问题：先生成一个扩展图G’，再对G’的每条边赋予单位流量1，然后求出最大流，就是该最大二分匹配问题的解。

  .. image:: images/max_flow4.png

.. literalinclude:: source/ford_fulkerson.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/ford_fulkerson.output



第七部分：其它的零散代码
==================================================

.. literalinclude:: source/computational_geometry.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/computational_geometry.output

.. literalinclude:: source/string_match.cpp
    :language: cpp
    :linenos:

.. literalinclude:: output/string_match.output



.. rubric:: 引用

.. [#] 《算法导论》第2版，机械工业出版社
.. [#] http://staff.ustc.edu.cn/~lszhuang/alg/
.. [#] http://www.wutianqi.com/
.. [#] http://www.cnblogs.com/timebug/
.. [#] http://blog.csdn.net/v_JULY_v/

如有参考而未能引用的，深表感谢与歉意！
